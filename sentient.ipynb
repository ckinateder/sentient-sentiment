{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "sentient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sentient-sentiment\n",
        "\n",
        "This is my research on developing a bidrectional LSTM model trained specifically for financial news.\n",
        "\n",
        "Datasets from [here](http://jmcauley.ucsd.edu/data/amazon/).\n",
        "\n",
        "*Author: Calvin Kinateder*"
      ],
      "metadata": {
        "id": "hvT5ODxlmD7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up drive support"
      ],
      "metadata": {
        "id": "QwtE8yKfwY6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NzrordpwZep",
        "outputId": "45f9eaa0-d4b1-48cc-8f93-0ec3a780b311"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Modules"
      ],
      "metadata": {
        "id": "E3u8knFnwS6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Importing required libraries\n",
        "!pip install jsonlines \n",
        "import nltk\n",
        "from keras import backend as K\n",
        "from scipy.interpolate import interp1d\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_absolute_error, mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
        "from sklearn.model_selection import train_test_split \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import jsonlines"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VsZEp_KwS6L",
        "outputId": "5dd7db06-ca4e-4d76-9c24-9bd3a8cb3129"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set params"
      ],
      "metadata": {
        "id": "8tyPfqZswS6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "NUM_WORDS = 8192 # number of unique words #(10000)\n",
        "LSTM_UNITS = 256 # lstm cells #(300)\n",
        "EMBED_OUT_DIM = 512\n",
        "NUM_CLASSES = 2 # is overwritten based on number of unique vals in data\n",
        "NUM_EPOCHS = 8 # keep lower to avoid overfitting\n",
        "TEXT_COL = \"text\"\n",
        "SCORE_COL = \"sentiment\"\n",
        "BATCH_SIZE = 512 #(400)\n",
        "TEST_SIZE = .2 # \n",
        "USING_TPU = True\n",
        "MAX_LENGTH = 0 # unknown at instantiation\n",
        "\n",
        "# get stop words\n",
        "stop_words = stopwords.words('english')"
      ],
      "outputs": [],
      "metadata": {
        "id": "pOjp6fQgwS6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up TPU\n",
        "Training happens much faster here."
      ],
      "metadata": {
        "id": "-rlTnvRNE0S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if USING_TPU:\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "  else:\n",
        "    TF_MASTER=''\n",
        "\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfgDpkD4EuDR",
        "outputId": "9b5ba199-c7ba-4902-9a11-b72052c6c2fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create persistent objects\n",
        "Tokenizer is used for both training and inference and thus must be kept the same."
      ],
      "metadata": {
        "id": "ZKkjqfYPwS6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokenizer = Tokenizer(num_words=NUM_WORDS, split=' ') \n",
        "labelEncoder=LabelEncoder()"
      ],
      "outputs": [],
      "metadata": {
        "id": "SPQ36U_OwS6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "imdb = pd.read_csv(\"/content/drive/MyDrive/datasets/imdb.csv\")[[\"review\",\"sentiment\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})"
      ],
      "outputs": [],
      "metadata": {
        "id": "QwkVZzlF7o0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load datasets"
      ],
      "metadata": {
        "id": "1AyfiqohDvSW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load jsonlines\n",
        "def read_jsonl(path:str)->pd.DataFrame:\n",
        "    out = []\n",
        "    with jsonlines.open(path) as f:\n",
        "        for line in f.iter():\n",
        "            out.append(line)\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def filter(df: pd.DataFrame, col: str):\n",
        "  df[col] = np.select(\n",
        "    [df[col] > 3, df[col] == 3, df[col] < 3], [\"positive\", \"neutral\", \"negative\"]\n",
        "  )"
      ],
      "outputs": [],
      "metadata": {
        "id": "MocG-UhnwS6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# from http://jmcauley.ucsd.edu/data/amazon/ \n",
        "amazon_set_1 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Musical_Instruments_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "filter(amazon_set_1, SCORE_COL)\n",
        "\n",
        "amazon_set_2 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Office_Products_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "filter(amazon_set_2, SCORE_COL)\n",
        "\n",
        "amazon_set_3 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Tools_and_Home_Improvement_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "filter(amazon_set_3, SCORE_COL)\n",
        "\n",
        "amazon_set_4 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Toys_and_Games_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "filter(amazon_set_4, SCORE_COL)\n",
        "\n",
        "amazon_set_5 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Home_and_Kitchen_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "filter(amazon_set_5, SCORE_COL)\n",
        "\n",
        "#amazon_set_6 = read_jsonl(\"/content/drive/MyDrive/datasets/reviews_Electronics_5.json\")[[\"summary\", \"overall\"]].rename(columns={\"summary\": TEXT_COL, \"overall\":SCORE_COL})\n",
        "#filter(amazon_set_6, SCORE_COL)\n",
        "\n",
        "imdb = pd.read_csv(\"/content/drive/MyDrive/datasets/imdb.csv\")[[\"review\",\"sentiment\"]].rename(columns={\"review\": TEXT_COL, \"sentiment\":SCORE_COL})\n",
        "\n",
        "#Loading the finance_set_1set\n",
        "finance_set_1 = pd.read_csv(\"/content/drive/MyDrive/datasets/small/all-data.csv\", encoding=\"ISO-8859-1\")\n",
        "finance_set_1.columns = [SCORE_COL, TEXT_COL]"
      ],
      "outputs": [],
      "metadata": {
        "id": "umnxEAoEwS6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create combined dataset"
      ],
      "metadata": {
        "id": "MpGngqO_CVLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = pd.concat([finance_set_1, amazon_set_1, amazon_set_2, amazon_set_3, amazon_set_4, amazon_set_5])\n",
        "data = data[data[SCORE_COL] != \"neutral\"] # remove neutral rows cause they do not matter\n",
        "data = data.sample(frac=1).reset_index(drop=True) # shuffle"
      ],
      "outputs": [],
      "metadata": {
        "id": "sqPSX1XACYgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Distribution"
      ],
      "metadata": {
        "id": "a6UXBqr54SrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def show_distribution(data: pd.DataFrame)->None:\n",
        "  pos_count = data[data[SCORE_COL] == 'positive'].shape[0]\n",
        "  neg_count = data[data[SCORE_COL] == 'negative'].shape[0]\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  ax1.pie([pos_count, neg_count], labels=[\"positive\", \"negative\"], autopct='%1.1f%%',\n",
        "          shadow=True, startangle=90)\n",
        "  ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "  plt.show()\n",
        "  print(f\"Data Positive {pos_count:,} / {data.shape[0]:,} ({(pos_count/data.shape[0])*100:.2f}%)\")\n",
        "  print(f\"Data Negative {neg_count:,} / {data.shape[0]:,} ({(neg_count/data.shape[0])*100:.2f}%)\")\n",
        "show_distribution(data)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "HvjB7n2G4YbQ",
        "outputId": "3df372fa-7ef7-4600-db9d-0a7c3ba255b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create equal distribution\n"
      ],
      "metadata": {
        "id": "kWUv5W0xQq_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def equalize_distribution(data: pd.DataFrame)->pd.DataFrame:\n",
        "  pos_count = data[data[SCORE_COL] == 'positive'].shape[0]\n",
        "  neg_count = data[data[SCORE_COL] == 'negative'].shape[0]\n",
        "  if pos_count > neg_count:\n",
        "    data[data[SCORE_COL] == 'positive'] = data[data[SCORE_COL] == 'positive'][-neg_count:]\n",
        "    data = data.dropna()\n",
        "  elif neg_count > pos_count:\n",
        "    data[data[SCORE_COL] == 'negative'] = data[data[SCORE_COL] == 'negative'][-pos_count:]\n",
        "    data = data.dropna()\n",
        "  return data\n",
        "\n",
        "#data = equalize_distribution(data)\n",
        "\n",
        "#show_distribution(data)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6mst8EYdQqa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean dataset"
      ],
      "metadata": {
        "id": "YBMmg6gPwS6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def clean(values: pd.Series)->pd.DataFrame: # improve this\n",
        "    # make all lowercase\n",
        "    values = values.apply(lambda x: str(x).lower())#' '.join(x.lower() for x in x.split()))\n",
        "    # Replacing the special characters, digits, numbers\n",
        "    values = values.apply(lambda x: re.sub('[^A-Za-z]+ ', ' ', x))\n",
        "    # remove html tags\n",
        "    values = values.apply(lambda x: re.sub('<.*?>', ' ', x))\n",
        "    # Lemmatization\n",
        "    values = values.apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
        "    return values\n",
        "    \n",
        "data[TEXT_COL] = clean(data[TEXT_COL])\n",
        "data[SCORE_COL].unique() # should only be \"negative\", \"positive\"\n",
        "data"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "hUwtZg5zwS6R",
        "outputId": "ddcf0b95-7c1e-4e6b-e058-509e302799ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode and transform sentiment\n",
        "This takes the unique values and converts into numbers."
      ],
      "metadata": {
        "id": "o9XuGUvuwS6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data[SCORE_COL] = labelEncoder.fit_transform(data[SCORE_COL])   "
      ],
      "outputs": [],
      "metadata": {
        "id": "OM4T71LvwS6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize words into X\n",
        "This, **with the same tokenzier**, is what will be used for processing text for inference."
      ],
      "metadata": {
        "id": "Qd2f2tbWwS6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# fit first\n",
        "tokenizer.fit_on_texts(data[TEXT_COL].values)\n",
        "\n",
        "def find_max_length(sequences: list)->int:\n",
        "  maxlen = 0\n",
        "  for seq in sequences:\n",
        "    if len(seq)> maxlen:\n",
        "      maxlen = len(seq)\n",
        "  return maxlen\n",
        "\n",
        "# define function to create X - this is used later\n",
        "def create_X(texts: pd.Series):\n",
        "  sequences = tokenizer.texts_to_sequences(clean(texts).values)\n",
        "  return pad_sequences(sequences, maxlen=MAX_LENGTH) # set max length here\n",
        "\n",
        "# create sequences\n",
        "sequences = tokenizer.texts_to_sequences(data[TEXT_COL].values)\n",
        "\n",
        "# find max length\n",
        "MAX_LENGTH = find_max_length(sequences)\n",
        "print(\"Max length is\", MAX_LENGTH)\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=MAX_LENGTH) # set max length here"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz4VNqc_wS6S",
        "outputId": "9aed7e8e-6c64-4171-cdba-4ac8c4e15214"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model\n",
        "\n",
        "### Layers\n",
        "**Embedding**: Turns positive integers (indexes) into dense vectors of fixed size. *NUM_WORDS* potential word values, *EMBED_OUT_DIM* max length.\n",
        "\n",
        "**SpatialDropout1D**: Helps reduce noise.\n",
        "\n",
        "**Bidirectional LSTM**: Most important layer. Is able to \"forget\" certain things and remember others. Bidirectional LSTMS can understand context better when compared to unidirectional LSTM\n",
        "\n",
        "**Dense**: Shapes into *NUM_CLASSES*, i.e. 0 for negative and 1 for positive.\n",
        "\n"
      ],
      "metadata": {
        "id": "vjF3NExRwS6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_model():  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS, EMBED_OUT_DIM, input_length = X.shape[1]))\n",
        "  model.add(SpatialDropout1D(0.4))\n",
        "  model.add(SpatialDropout1D(0.3))\n",
        "  model.add(Bidirectional(LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "  model.add(LSTM(int(LSTM_UNITS/2), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "  model.add(LSTM(int(LSTM_UNITS/4), dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def compile_model(model: Sequential): #categorical_crossentropy for multiple category flex\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "NUM_CLASSES = data[SCORE_COL].nunique() # reset cols\n",
        "if USING_TPU:\n",
        "  with strategy.scope():\n",
        "    model = create_model()\n",
        "    compile_model(model)\n",
        "else:\n",
        "    model = create_model()\n",
        "    compile_model(model)\n",
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tow4khd8wS6T",
        "outputId": "74ee81fd-9707-4d98-cb2e-4152ccb41da2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into train and test"
      ],
      "metadata": {
        "id": "P11Gyn8ewS6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Splitting the finance_set_1 into training and testing\n",
        "y=pd.get_dummies(data[SCORE_COL])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=TEST_SIZE, random_state = 42, )"
      ],
      "outputs": [],
      "metadata": {
        "id": "7av5C4oxwS6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit model\n",
        "Depending on the system running, this can take an absurd amount of time."
      ],
      "metadata": {
        "id": "7QXHz9tUwS6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.fit(X_train, y_train, epochs = NUM_EPOCHS, batch_size=BATCH_SIZE, verbose = 1)\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwcynw-4wS6U",
        "outputId": "61843b56-41b3-447f-9c60-7298f20963d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model\n",
        "View subsets and evaluate"
      ],
      "metadata": {
        "id": "H3H6IWFlwS6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc_and_loss = model.evaluate(X_test,y_test)\n",
        "loss = acc_and_loss[0]\n",
        "accuracy = acc_and_loss[1]\n",
        "print(f\"Model loss on test set with {y_test.shape[0]} rows: {loss:.4f}\")\n",
        "print(f\"Model accuracy on test set with {y_test.shape[0]} rows: {accuracy*100:0.2f}%\")\n",
        "y_pred = model.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dH1zTAvswS6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "subset_x = X_test[-10:]\n",
        "subset_y = y_test[-10:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "sTYnIVrpW252"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "subset_y_pred = model.predict(subset_x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "o-Y4QPcGkr1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "subset_y_pred.round()"
      ],
      "outputs": [],
      "metadata": {
        "id": "pQ86xb8TXFBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "subset_y"
      ],
      "outputs": [],
      "metadata": {
        "id": "5USF41iATL3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "subset_x"
      ],
      "outputs": [],
      "metadata": {
        "id": "tIVnQhe3XLJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def score(*args)->dict: # predict score for a text input, and return % positive\n",
        "  interp = interp1d([0,1],[-1,1])\n",
        "  values = {}\n",
        "  y = model.predict(create_X(pd.Series([*args])))\n",
        "  for i in range(len(args)):\n",
        "    values[args[i]] = round(float(interp(y[i][1])), 4)\n",
        "  return values\n",
        "\n",
        "\n",
        "score(\"Apple's sales dropped today\", \"the most amazing product\", \"your mom stinks\", \"i love your mom\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "C9au_CyeXMSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.save(\"/content/drive/MyDrive/sentient-sentiment-v1\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "lLG3wRjmnvFw"
      }
    }
  ]
}